---
title: "LLM Reasoning Chain Analysis with Inseq"
collection: portfolio
excerpt: "Token- and step-level analysis of reasoning behavior in large language models."
---

### Overview
This project focused on analyzing and understanding the internal reasoning behavior of large language models using attribution and explainability tools.

### What I Did
- Analyzed reasoning chains and intermediate token contributions in LLM outputs
- Used **Inseq** to inspect token-level attributions across reasoning steps
- Compared how different prompts, decoding strategies, and model settings affect reasoning behavior
- Studied failure modes such as hallucination, shortcut reasoning, and reasoning collapse

### Tools & Methods
- Inseq for attribution analysis  
- Transformer-based LLMs  
- Prompt and decoding ablations  
- Qualitative and quantitative reasoning inspection

### Why It Matters
This work focused on **interpretability and trust in LLMs**, helping bridge the gap between model outputs and their underlying decision processes.
